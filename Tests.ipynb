{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn' \n",
    "# Ignoring SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nii_to_array(nii_path):\n",
    "    \n",
    "    \"\"\" \n",
    "    Function returns np.array data from the *.nii file\n",
    "    \n",
    "    Arguments:\n",
    "        * nii_path (str): path to *.nii file with data\n",
    "\n",
    "    Output:\n",
    "        * result (np.array): data obtained from nii_path\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = np.asanyarray(nib.load(nii_path).dataobj)\n",
    "        return (result)\n",
    "    \n",
    "    except OSError:\n",
    "        print(FileNotFoundError(f'No such file or no access: {nii_path}'))\n",
    "        return('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crope_image(img, coord_min, img_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function cropes an image \n",
    "    \n",
    "    Arguments:\n",
    "        * img (np.array): MR image\n",
    "        * coord_min (x_min, y_min, z_min):  the most bottom coordinate\n",
    "        * img_shape (x_shape, y_shape, z_shape): desired image shape\n",
    "\n",
    "    Output:\n",
    "        * img (np.array): croped image with size (1, x_shape, y_shape, z_shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    img = img[coord_min[0]:coord_min[0] + img_shape[0],\n",
    "              coord_min[1]:coord_min[1] + img_shape[1],\n",
    "              coord_min[2]:coord_min[2] + img_shape[2],]\n",
    "    \n",
    "    if img.shape[:3] != img_shape:\n",
    "        print(f'Current image shape: {img.shape[:3]}')\n",
    "        print(f'Desired image shape: {img_shape}')\n",
    "        raise AssertionError\n",
    "        \n",
    "    return img.reshape((1,) + img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_info(sample, \n",
    "                     targets_path = 'targets/targets_fcd_bank.csv',\n",
    "                     image_path='../../datasets/fcd_classification_bank',\n",
    "                     mask_path = False,\n",
    "                     prefix = False,\n",
    "                     data_type = False,\n",
    "                     ignore_missing = True):\n",
    "    \n",
    "    '''\n",
    "    Function to obtain DataFrame with all the information about\n",
    "    needed MR images (including paths to MRI and to file with ground truth segmentation).\n",
    "    Walks through directories and completes DataFrame, according to targets.\n",
    "    \n",
    "    Arguments:\n",
    "        * targets_path (str): path to DataFrame with all the information about MR images.\n",
    "        * sample (str): name of the medical center with images from which we want to work, 'all' for whole centers\n",
    "        * prefix (str): patient name prefix (optional). In case we want to work e.g only with fcd or no_fcd patients.\n",
    "        * mask_path (str or False): paths to mask folders \n",
    "        * image_path (str): path to nii.gz files with MR data\n",
    "        * data_type (str or False): str - 'img' or'seg'. If e.g data_type = 'img' - only MR image is used for model.\n",
    "        'img' or 'seg' are using in classification, in segmentation data_type = False, since we want to have both. \n",
    "        * ignore_missing (bool): whther we want to remove examples with missing data or not.\n",
    "    \n",
    "    Outputs:\n",
    "        * files: DataFrame with all the information about targets, which are needed for our task\n",
    "        (including paths to MRI and to file with ground truth segmentation)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    description_of_targets = pd.read_csv(targets_path, index_col = 0)\n",
    "    '''\n",
    "    DataFrame with infromation about each MRI\n",
    "        * sample: 'pirogov', 'la5_study', 'soloviev', 'hcp', 'kulakov'. Names of medical centers from which MR images were obtained\n",
    "        * patien: names of MR images \n",
    "        * fcd: 1 or 0. Responsible for the presence or absence of the FCD in MR images  \n",
    "        * age: ages of patients, whose MR images we have\n",
    "        * gender: gender of patients, whose MR images we have\n",
    "        * scan: . To be honest don't know \n",
    "        * detection: ['mri_positive', 'mri_negative', nan, 'mri_positive/mri_negative']. To be honest don't know\n",
    "        * comments: [nan,  0.,  3.,  2.,  1.]. To be honest don't know\n",
    "    '''\n",
    "    \n",
    "    files = pd.DataFrame(columns = ['patient','scan','fcd','img_file','img_seg']) # output DataFrame\n",
    "    condition_about_sample = (description_of_targets['sample'] == sample)\n",
    "                         \n",
    "    if prefix:\n",
    "        condition_about_sample = (description_of_targets['sample'] == sample)&(description_of_targets['patient'].str.startswith(prefix))\n",
    "        \n",
    "    files['patient'] = description_of_targets['patient'][condition_about_sample].copy()\n",
    "    files['fcd'] = description_of_targets['fcd'][condition_about_sample].copy()\n",
    "    files['scan'] = description_of_targets['scan'][condition_about_sample].copy()\n",
    "    files['detection'] = description_of_targets['detection'][condition_about_sample].copy()\n",
    "    files['comments'] = description_of_targets['comments'][condition_about_sample].copy()\n",
    "    \n",
    "    if mask_path:\n",
    "        files['img_mask'] = ''\n",
    "    \n",
    "    elif sample == 'all':\n",
    "        files['patient'] = description_of_targets['patient'].copy()\n",
    "        files['fcd'] = description_of_targets['fcd'].copy()\n",
    "        files['scan'] = description_of_targets['scan'].copy()\n",
    "        files['detection'] = description_of_targets['detection'].copy()\n",
    "        files['comments'] = description_of_targets['comments'].copy()\n",
    "        \n",
    "    # paths to MRI and corresponding segmentation are adding\n",
    "    for i in tqdm(range(len(files))):\n",
    "        for file_in_folder in glob.glob(os.path.join(image_path,'*norm*')):\n",
    "            if sample == 'pirogov':\n",
    "                if (files['patient'].iloc[i] + '_norm.nii.gz') == file_in_folder.split('/')[-1]:\n",
    "                    files['img_file'].iloc[i] = file_in_folder\n",
    "            else:\n",
    "                if files['patient'].iloc[i] in file_in_folder:\n",
    "                    files['img_file'].iloc[i] = file_in_folder\n",
    "\n",
    "        for file_in_folder in glob.glob(os.path.join(image_path,'*aseg*')):\n",
    "            if sample == 'pirogov':\n",
    "                if ((files['patient'].iloc[i] +'_aparc+aseg.nii.gz') == file_in_folder.split('/')[-1]) or\\\n",
    "                ((files['patient'].iloc[i] +'_aparc+aseg.nii') == file_in_folder.split('/')[-1]):\n",
    "                    files['img_seg'].iloc[i] = file_in_folder \n",
    "            else:    \n",
    "                if files['patient'].iloc[i] in file_in_folder:\n",
    "                    files['img_seg'].iloc[i] = file_in_folder   \n",
    "\n",
    "        if mask_path:\n",
    "            for file_in_folder in glob.glob(os.path.join(mask_path,'*.nii*')):\n",
    "                if (files['patient'].iloc[i] +'.nii.gz') == file_in_folder.split('/')[-1]:\n",
    "                    files['img_mask'].iloc[i] = file_in_folder \n",
    "    \n",
    "    # treating missing data\n",
    "    if ignore_missing:\n",
    "        # if only 'img' is needed for classification\n",
    "        if data_type =='img':\n",
    "            files.dropna(subset = ['img_file'], inplace= True)\n",
    "        # if only 'seg' is needed for classification\n",
    "        elif data_type =='seg':\n",
    "            files.dropna(subset = ['img_seg'], inplace= True)\n",
    "        # saving only full pairs of data. Mandatory for segmentation \n",
    "        else: \n",
    "            files.dropna(subset = ['img_seg','img_file'], inplace= True)\n",
    "\n",
    "    # reindexing an array, since we droped NaNs.\n",
    "    files = files.reset_index(drop=True)\n",
    "    label_encoder = LabelEncoder() \n",
    "    files['scan'] = label_encoder.fit_transform(files['scan'])\n",
    "\n",
    "    return files, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data.Dataset is an abstract class representing a :class:`Dataset`.\n",
    "\n",
    "All datasets that represent a map from keys to data samples should subclass\n",
    "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
    "data sample for a given key. Subclasses could also optionally overwrite\n",
    ":meth:`__len__`.\n",
    "\"\"\"\n",
    "\n",
    "class DataMriSegmentation(data.Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image_path (str): paths to data folders  \n",
    "        mask_path (str): paths to mask folders  \n",
    "        prefix (str): patient name prefix (optional)\n",
    "        sample (str): subset of the data, 'all' for whole sample\n",
    "        targets_path (str): targets file path\n",
    "        if ignore_missing (bool): delete subject if the data partially missing\n",
    "        mask (string): ['seg', 'bb', 'combined']    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample,\n",
    "                 image_path = '../../datasets/fcd_classification_bank',\n",
    "                 targets_path = 'targets/targets_fcd_bank.csv',\n",
    "                 mask_path = False,\n",
    "                 prefix = False,\n",
    "                 ignore_missing = True,\n",
    "                 coord_min = (30,30,30, ),\n",
    "                 img_shape = (192, 192, 192, ),\n",
    "                 mask = 'seg'):\n",
    "        \n",
    "        super(DataMriSegmentation, self).__init__()\n",
    "        \n",
    "        print(f'Assembling data for: {sample} sample.')\n",
    "        files, label_encoder = get_targets_info(sample, targets_path, image_path,\n",
    "                                 mask_path, prefix, ignore_missing)\n",
    "        \n",
    "        self.img_files = files['img_file']\n",
    "        self.img_seg = files['img_seg']\n",
    "        self.scan = files['scan']\n",
    "        self.scan_keys = label_encoder.classes_\n",
    "        self.target = files['fcd'] \n",
    "        self.detection = files['detection']\n",
    "        self.misc = files['comments']\n",
    "        \n",
    "        if mask_path:\n",
    "            self.img_mask = files['img_mask']\n",
    "            \n",
    "        self.coord_min = coord_min\n",
    "        self.img_shape = img_shape\n",
    "        self.mask_path = mask_path\n",
    "        self.mask = mask\n",
    "        \n",
    "        assert mask in ['seg','bb','combined'], \"Invalid mask name!\"\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            seg_path = self.img_seg[index]\n",
    "            \n",
    "            img_array = load_nii_to_array(img_path)\n",
    "            seg_array = load_nii_to_array(seg_path)\n",
    "                                   \n",
    "            img = crope_image(img_array, self.coord_min, self.img_shape)\n",
    "            seg = crope_image(seg_array, self.coord_min, self.img_shape)\n",
    "                                   \n",
    "            if self.mask == 'seg':\n",
    "                # binarising cortical structures\n",
    "                seg[seg < 1000] = 0\n",
    "                seg[seg > 1000] = 1\n",
    "                return torch.from_numpy(img).float(), torch.from_numpy(seg).float()\n",
    "\n",
    "            elif self.mask == 'bb':\n",
    "                # preparing bounding box mask \n",
    "                bb_mask_path = self.img_mask[index]\n",
    "                mask_array = load_nii_to_array(bb_mask_path)\n",
    "                masked_img = crope_image(mask_array, self.coord_min, self.img_shape)\n",
    "                return torch.from_numpy(img).float(), torch.from_numpy(masked_img).float()\n",
    "            \n",
    "            elif self.mask == 'combined':\n",
    "                # binarising cortical structures\n",
    "                seg[seg < 1000] = 0\n",
    "                seg[seg > 1000] = 1\n",
    "                \n",
    "                # preparing bounding box mask \n",
    "                bb_mask_path = self.img_mask[index]\n",
    "                mask_array = load_nii_to_array(bb_mask_path)\n",
    "                masked_img = crope_image(mask_array, self.coord_min, self.img_shape)\n",
    "                \n",
    "                # calculating combined mask as intersection of both masks\n",
    "                comb_mask = np.logical_and(masked_img, seg)\n",
    "                return torch.from_numpy(img).float(), torch.from_numpy(comb_mask).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_central_cuts(img):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function plots central slices of MRI\n",
    "    \n",
    "    Arguments:\n",
    "        * img (torch.Tensor): MR image (1xDxHxW)\n",
    "    \n",
    "    Output:\n",
    "        * picture of central slices of MRI\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.numpy()\n",
    "        if (len(img.shape) > 3):\n",
    "            img = img[0,:,:,:]\n",
    "                \n",
    "    elif isinstance(img, nibabel.nifti1.Nifti1Image):    \n",
    "        img = img.get_fdata()\n",
    "   \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(3 * 4, 4))\n",
    "    \n",
    "    axes[0].imshow(img[ img.shape[0] // 2, :, :])\n",
    "    axes[1].imshow(img[ :, img.shape[1] // 2, :])\n",
    "    axes[2].imshow(img[ :, :, img.shape[2] // 2])\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_img(img, cmap = 'gray'):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.show()\n",
    "\n",
    "def create_slider(img):\n",
    "    sld_sagital = IntSlider(min=0, max=img.shape[0]-1, step=1, continuous_update=True)\n",
    "    sld_coronal = IntSlider(min=0, max=img.shape[1]-1, step=1, continuous_update=True)\n",
    "    sld_axial   = IntSlider(min=0, max=img.shape[2]-1, step=1, continuous_update=True)\n",
    "    return sld_sagital, sld_coronal, sld_axial\n",
    "\n",
    "\n",
    "def vary_coordinate(coordinate_sagital, coordinate_coronal, coordinate_axial, axis = None):\n",
    "    if axis == None:\n",
    "        pass\n",
    "    if axis == 'sagital':\n",
    "        plot_img(img[coordinate_sagital,:,:])\n",
    "    if axis == 'coronal':\n",
    "        plot_img(img[:,coordinate_coronal ,:])\n",
    "    if axis == 'axial':\n",
    "        plot_img(img[:,:, coordinate_axial])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_nii_to_array('../../datasets/fcd_classification_bank/fcd_0_1_norm.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --sys-prefix --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAMN0lEQVR4nO3cT4yc9X3H8fenOOFAkICQWq5xC4mcg3NxrBVFKorSQxPgYnJB5FCsCsk5gJRI6cFJDuXaqkkk1BTJUVBMlUKREoQP/ROwItELBBsRY0MJJjHClrEbURHUSkmAbw/7mEz89Xpnd2d2Ztv3SxrN7G+f2fkyMm89zzN/UlVI0qjfm/UAkuaPYZDUGAZJjWGQ1BgGSY1hkNRMLQxJbknycpITSfZN63EkTV6m8T6GJJcBPwX+DDgFPAt8vqpenPiDSZq4ae0x3AicqKqfVdWvgUeA3VN6LEkTtmlKf3cr8PrIz6eAP15q4yS+/VKavl9U1UfG2XBaYVhWkr3A3lk9vvT/0GvjbjitMJwGto38fN2w9r6q2g/sB/cYpHkzrXMMzwLbk9yQ5IPAncDBKT2WpAmbyh5DVb2T5F7g34DLgAer6vg0HkvS5E3l5coVD+GhhLQejlTVwjgb+s5HSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUbFrLnZOcBN4G3gXeqaqFJNcA/wRcD5wE7qiq/1rbmJLW0yT2GP60qnZW1cLw8z7gUFVtBw4NP0vaQKZxKLEbODDcPgDcPoXHkDRFaw1DAT9MciTJ3mFtc1WdGW6/AWy+2B2T7E1yOMnhNc4gacLWdI4BuLmqTif5feCJJP8x+suqqiR1sTtW1X5gP8BS20iajTXtMVTV6eH6HPAYcCNwNskWgOH63FqHlLS+Vh2GJFckufL8beAzwDHgILBn2GwP8Phah5S0vtZyKLEZeCzJ+b/zj1X1r0meBR5NcjfwGnDH2seUtJ5SNfvDe88xSOviyMjbCi7Jdz5KagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGqWDUOSB5OcS3JsZO2aJE8keWW4vnpYT5L7k5xIcjTJrmkOL2k6xtlj+C5wywVr+4BDVbUdODT8DHArsH247AUemMyYktbTsmGoqqeANy9Y3g0cGG4fAG4fWX+oFj0NXJVky6SGlbQ+VnuOYXNVnRluvwFsHm5vBV4f2e7UsCZpA9m01j9QVZWkVnq/JHtZPNyQNGdWu8dw9vwhwnB9blg/DWwb2e66Ya2pqv1VtVBVC6ucQdKUrDYMB4E9w+09wOMj63cNr07cBLw1csghaaOoqktegIeBM8BvWDxncDfwYRZfjXgFeBK4Ztg2wLeAV4EXgIXl/v5wv/LixcvUL4fH+f+xqsjwP+ZMreYchaQVOzLuobvvfJTUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSc2yYUjyYJJzSY6NrN2X5HSS54fLbSO/+0qSE0leTvLZaQ0uaXrG2WP4LnDLRda/WVU7h8s/AyTZAdwJfGK4z98nuWxSw0paH8uGoaqeAt4c8+/tBh6pql9V1c+BE8CNa5hP0gys5RzDvUmODocaVw9rW4HXR7Y5Naw1SfYmOZzk8BpmkDQFqw3DA8DHgJ3AGeDrK/0DVbW/qhaqamGVM0iaklWFoarOVtW7VfUe8G1+e7hwGtg2sul1w5qkDWRVYUiyZeTHzwHnX7E4CNyZ5PIkNwDbgR+vbURJ623TchskeRj4NHBtklPAXwGfTrITKOAk8AWAqjqe5FHgReAd4J6qenc6o0uallTVrGcgyeyHkP7vOzLuOT3f+SipMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqVk2DEm2JflRkheTHE/yxWH9miRPJHlluL56WE+S+5OcSHI0ya5p/0dImqxx9hjeAb5cVTuAm4B7kuwA9gGHqmo7cGj4GeBWYPtw2Qs8MPGpJU3VsmGoqjNV9dxw+23gJWArsBs4MGx2ALh9uL0beKgWPQ1clWTLxCeXNDUrOseQ5Hrgk8AzwOaqOjP86g1g83B7K/D6yN1ODWuSNohN426Y5EPA94EvVdUvk7z/u6qqJLWSB06yl8VDDUlzZqw9hiQfYDEK36uqHwzLZ88fIgzX54b108C2kbtfN6z9jqraX1ULVbWw2uElTcc4r0oE+A7wUlV9Y+RXB4E9w+09wOMj63cNr07cBLw1csghaQNI1aWPAJLcDPw78ALw3rD8VRbPMzwK/CHwGnBHVb05hOTvgFuA/wH+oqoOL/MYKzoMkbQqR8bdQ182DOvBMEjrYuww+M5HSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDULBuGJNuS/CjJi0mOJ/nisH5fktNJnh8ut43c5ytJTiR5Oclnp/kfIGnyNo2xzTvAl6vquSRXAkeSPDH87ptV9bejGyfZAdwJfAL4A+DJJB+vqncnObik6Vl2j6GqzlTVc8Ptt4GXgK2XuMtu4JGq+lVV/Rw4Adw4iWElrY8VnWNIcj3wSeCZYeneJEeTPJjk6mFtK/D6yN1OcZGQJNmb5HCSwyueWtJUjR2GJB8Cvg98qap+CTwAfAzYCZwBvr6SB66q/VW1UFULK7mfpOkbKwxJPsBiFL5XVT8AqKqzVfVuVb0HfJvfHi6cBraN3P26YU3SBjHOqxIBvgO8VFXfGFnfMrLZ54Bjw+2DwJ1JLk9yA7Ad+PHkRpY0beO8KvEnwJ8DLyR5flj7KvD5JDuBAk4CXwCoquNJHgVeZPEVjXt8RULaWFJVs56BJP8J/Dfwi1nPMoZr2RhzwsaZ1Tkn72Kz/lFVfWScO89FGACSHN4IJyI3ypywcWZ1zslb66y+JVpSYxgkNfMUhv2zHmBMG2VO2DizOufkrWnWuTnHIGl+zNMeg6Q5MfMwJLll+Hj2iST7Zj3PhZKcTPLC8NHyw8PaNUmeSPLKcH31cn9nCnM9mORckmMjaxedK4vuH57jo0l2zcGsc/ex/Ut8xcBcPa/r8lUIVTWzC3AZ8CrwUeCDwE+AHbOc6SIzngSuvWDtb4B9w+19wF/PYK5PAbuAY8vNBdwG/AsQ4CbgmTmY9T7gLy+y7Y7h38HlwA3Dv4/L1mnOLcCu4faVwE+Heebqeb3EnBN7Tme9x3AjcKKqflZVvwYeYfFj2/NuN3BguH0AuH29B6iqp4A3L1heaq7dwEO16Gngqgve0j5VS8y6lJl9bL+W/oqBuXpeLzHnUlb8nM46DGN9RHvGCvhhkiNJ9g5rm6vqzHD7DWDzbEZrlpprXp/nVX9sf9ou+IqBuX1eJ/lVCKNmHYaN4Oaq2gXcCtyT5FOjv6zFfbW5e2lnXucasaaP7U/TRb5i4H3z9LxO+qsQRs06DHP/Ee2qOj1cnwMeY3EX7Oz5Xcbh+tzsJvwdS801d89zzenH9i/2FQPM4fM67a9CmHUYngW2J7khyQdZ/K7IgzOe6X1Jrhi+55IkVwCfYfHj5QeBPcNme4DHZzNhs9RcB4G7hrPoNwFvjewaz8Q8fmx/qa8YYM6e16XmnOhzuh5nUZc5w3obi2dVXwW+Nut5Lpjtoyyezf0JcPz8fMCHgUPAK8CTwDUzmO1hFncXf8PiMePdS83F4lnzbw3P8QvAwhzM+g/DLEeHf7hbRrb/2jDry8Ct6zjnzSweJhwFnh8ut83b83qJOSf2nPrOR0nNrA8lJM0hwyCpMQySGsMgqTEMkhrDIKkxDJIawyCp+V+HGI3joF+xpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sld_sagital = IntSlider(min=0, max=img.shape[0]-1, step=1, continuous_update=True)\n",
    "sld_coronal = IntSlider(min=0, max=img.shape[1]-1, step=1, continuous_update=True)\n",
    "sld_axial   = IntSlider(min=0, max=img.shape[2]-1, step=1, continuous_update=True)\n",
    "\n",
    "interact(vary_coordinate,\n",
    "         coordinate_axial = sld_axial,\n",
    "         coordinate_sagital = sld_sagital,\n",
    "         coordinate_coronal = sld_coronal,\n",
    "         axis = 'axial');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel\n",
    "\n",
    "def plot_central_cuts(img, title=\"\"):\n",
    "    \"\"\"\n",
    "    param image: tensor or np array of shape (CxDxHxW) if t is None\n",
    "    \"\"\"\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.numpy()\n",
    "        if (len(img.shape) > 3):\n",
    "            img = img[0,:,:,:]\n",
    "                \n",
    "    elif isinstance(img, nibabel.nifti1.Nifti1Image):    \n",
    "        img = img.get_fdata()\n",
    "   \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(3 * 4, 4))\n",
    "    axes[0].imshow(img[ img.shape[0] // 2, :, :])\n",
    "    axes[1].imshow(img[ :, img.shape[1] // 2, :])\n",
    "    axes[2].imshow(img[ :, :, img.shape[2] // 2])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_predicted(img, seg, delta = 0, title=\"\"):\n",
    "    \"\"\"\n",
    "    param image: tensor or np array of shape (CxDxHxW) if t is None\n",
    "    \"\"\"\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.cpu().numpy()\n",
    "        if (len(img.shape) == 5):\n",
    "            img = img[0,0,:,:,:]\n",
    "        elif (len(img.shape) == 4):\n",
    "            img = img[0,:,:,:]\n",
    "                \n",
    "    elif isinstance(img, nibabel.nifti1.Nifti1Image):    \n",
    "        img = img.get_fdata()\n",
    "        \n",
    "    if isinstance(seg, torch.Tensor):\n",
    "        seg= seg[0].cpu().numpy().astype(np.uint8)\n",
    "   \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(3 * 4, 4))\n",
    "    axes[0].imshow(img[ img.shape[0] // 2 + delta, :, :])\n",
    "    axes[1].imshow(seg[ seg.shape[0] // 2 + delta, :, :])\n",
    "    intersect = img[ img.shape[0] // 2 + delta, :, :] + seg[ seg.shape[0] // 2 + delta, :, :]*100\n",
    "    axes[2].imshow(intersect, cmap='gray')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_difference(img, seg, delta = 0, title=\"\"):\n",
    "    \"\"\"\n",
    "    param image: tensor or np array of shape (CxDxHxW) if t is None\n",
    "    \"\"\"\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.cpu().numpy()\n",
    "        if (len(img.shape) == 5):\n",
    "            img = img[0,0,:,:,:]\n",
    "        elif (len(img.shape) == 4):\n",
    "            img = img[0,:,:,:]\n",
    "                \n",
    "    elif isinstance(img, nibabel.nifti1.Nifti1Image):    \n",
    "        img = img.get_fdata()\n",
    "        \n",
    "    if isinstance(seg, torch.Tensor):\n",
    "        seg= seg[0].cpu().numpy().astype(np.uint8)\n",
    "   \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(3 * 4, 4))\n",
    "    axes[0].imshow(img[ img.shape[0] // 2 + delta, :, :])\n",
    "    axes[1].imshow(seg[ seg.shape[0] // 2 + delta, :, :])\n",
    "    intersect = (img[ img.shape[0] // 2 + delta, :, :] - seg[ seg.shape[0] // 2 + delta, :, :])*100\n",
    "    axes[2].imshow(intersect, cmap='gray')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
